{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18b1dca8",
      "metadata": {
        "id": "18b1dca8"
      },
      "source": [
        "# Chapter 1 - ML Basics Lab — Pandas, Logistic Regression, Decision Trees\n",
        "\n",
        "**This week's exercise has 4 tasks for a total of 5 points. Every task you need to do from now on will look like this:**\n",
        "\n",
        "**Task X (Y points)**: Jump up and down 3 times and then print \"hello world\".\n",
        "\n",
        "For Programming tasks, code your solutions, test them, and put them on GitHub. To hand them in, demonstrate your solution to a tutor.\n",
        "For Understanding questions, you can write down your answers to help remember them for the exam, but you don't have to. To hand them in, present them to a tutor.\n",
        "\n",
        "**Don't forget to commit your solutions to GitHub!**\n",
        "\n",
        "Before we can dive into modern machine learning and more complicated neural networks, we will explore some of the older machine learning strategies. They are robust, and comparatively easy to understand and implement, but pale in comparison with the performance of modern methods.\n",
        "\n",
        "- Handling data using Pandas\n",
        "- Simple data visualizations\n",
        "- Using and understanding simple ML models like Logistic Regression and Decision Trees in scikit-learn\n",
        "\n",
        "**Dataset:** As our dataset, we will use the Breast Cancer Wisconsin (Diagnostic) dataset — this is built into `scikit-learn`, with no download needed.  \n",
        "**Flow:** The order of topics in this notebook looks something like this: Load data → Inspect data → Split data into datasets → Baselines → Logistic Regression → Decision Tree → Comparison → Hyperparameter Tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289b5849",
      "metadata": {
        "id": "289b5849"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Imports ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix,\n",
        "    RocCurveDisplay, ConfusionMatrixDisplay, PrecisionRecallDisplay, brier_score_loss,\n",
        "    DetCurveDisplay\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "RNG = 42\n",
        "np.random.seed(RNG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd441364",
      "metadata": {
        "id": "fd441364"
      },
      "source": [
        "## Pandas\n",
        "\n",
        "Pandas is a Python package intended for managing tabular data in an object called a DataFrame. You can think of a DataFrame as kind of similar to a NumPy array or a two-dimensional Dictionary, except with a lot more features - we can sort the entire table by rows or columns very easily, throw out or add rows and columns, etc. etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd1ff7a",
      "metadata": {
        "id": "4bd1ff7a"
      },
      "source": [
        "## Load dataset as a pandas DataFrame\n",
        "\n",
        "The dataset we will work with this week is included in scikit-learn and can therefore be downloaded via the _sklearn.datasets_ functions. We have already imported the necessary function at the start (`from sklearn.datasets import load_breast_cancer`).\n",
        "\n",
        "The dataset contains information (\"features\") on cell nuclei present in images taken of fine needle aspirates (FNAs) of breast masses. Our goal will be to use this data later to try and predict whether those masses represent benign growths or tumors, using some older machine learning approaches that are simpler, and more easily explainable, than neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ef689a",
      "metadata": {
        "id": "09ef689a"
      },
      "outputs": [],
      "source": [
        "# Let's first load our data - then, with the help of that data, we will learn to navigate Pandas a little better\n",
        "\n",
        "data = load_breast_cancer(as_frame=True)\n",
        "X = data.data.copy()\n",
        "y = pd.Series(data.target, name=\"target\")  # 0 = malignant, 1 = benign\n",
        "df = X.join(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627db25c",
      "metadata": {
        "id": "627db25c"
      },
      "outputs": [],
      "source": [
        "# df.head() shows us the top part (the \"head\") of our table\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503fc5cc",
      "metadata": {
        "id": "503fc5cc"
      },
      "outputs": [],
      "source": [
        "# df.info() shows us some general information on our table\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdca2c21",
      "metadata": {
        "id": "fdca2c21"
      },
      "outputs": [],
      "source": [
        "# We can select a single column like we would in a Dict\n",
        "print(df[\"mean radius\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0c4f21",
      "metadata": {
        "id": "8f0c4f21"
      },
      "outputs": [],
      "source": [
        "# We can also nicely select parts of our table based on some condition.\n",
        "# For example, we could keep all rows where mean radius is at least 15:\n",
        "large_radius = df[df[\"mean radius\"] > 15]\n",
        "print(large_radius.head())\n",
        "# The syntax for this can be understood as follows:\n",
        "# My new table (large_radius) is = everywhere in my old table (df) where\n",
        "# a specific condition (df[\"mean radius\" > 15]) is met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168f9acf",
      "metadata": {
        "id": "168f9acf"
      },
      "outputs": [],
      "source": [
        "# Similar to a List or Array, we can also iterate over a pandas dataframe:\n",
        "# For column names:\n",
        "for col in df:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tzRc2biiz6Az",
      "metadata": {
        "id": "tzRc2biiz6Az"
      },
      "outputs": [],
      "source": [
        "# To look at one column only, for example \"mean radius\":\n",
        "print(df[\"mean radius\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BnCtDMpBytoW",
      "metadata": {
        "id": "BnCtDMpBytoW"
      },
      "outputs": [],
      "source": [
        "# ... and for rows:\n",
        "for n, row in df.iterrows():\n",
        "    print(row)\n",
        "    if n == 3:\n",
        "        break # We break off the for-loop early, so we don't print at all the rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf5eb80",
      "metadata": {
        "id": "fcf5eb80"
      },
      "outputs": [],
      "source": [
        "# We can also easily add data to a pandas dataframe\n",
        "\n",
        "shape = df.shape\n",
        "num_rows, num_columns = shape[0], shape[1]\n",
        "new_column_name = \"example1\"\n",
        "new_column_data = [n for n in range(num_rows)]\n",
        "\n",
        "# We can manually make new data and add it ...\n",
        "df[new_column_name] = new_column_data\n",
        "\n",
        "# ... or with the help of an already existing column\n",
        "df[\"example2\"] = df[\"example1\"] + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc406633",
      "metadata": {
        "id": "cc406633"
      },
      "outputs": [],
      "source": [
        "# You can see the new columns appended to the end\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62f9bf6",
      "metadata": {
        "id": "c62f9bf6"
      },
      "source": [
        "## Quick exploration of the dataset with pandas\n",
        "\n",
        "Let us quickly look at our data. How many features, how many examples does it have? Is there a class imbalance? What features do we have available?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4266c1",
      "metadata": {
        "id": "dd4266c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nClass balance:\")\n",
        "print(y.value_counts(normalize=True).rename({0: \"malignant\", 1: \"benign\"}))\n",
        "\n",
        "print(\"\\nDescribe (numeric):\")\n",
        "display(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548ec2c5",
      "metadata": {
        "id": "548ec2c5"
      },
      "source": [
        "### Missing values\n",
        "\n",
        "Sometimes, data can be missing from tables. In this case, the table does not contain a zero or something like that, it contains a \"NaN\" (\"Not a Number\"). What is the average of 10, 3, and not a number? Nothing, of course - we can't calculate that. A lot of calculations - and therefore machine learning - is going to break, if we leave these holes in our dataset.\n",
        "\n",
        "Let's check if there are any missing values in our dataset, using the `.isna()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ef72aa",
      "metadata": {
        "id": "39ef72aa"
      },
      "outputs": [],
      "source": [
        "# Check for NaNs, sum them up, and sort\n",
        "na_counts = df.isna().sum().sort_values(ascending=False)\n",
        "# Keep only the parts of the table where we counted at least 1 NaN and print it\n",
        "print(na_counts[na_counts > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e06668",
      "metadata": {
        "id": "58e06668"
      },
      "source": [
        "The result is empty, which means there was no place with any NaNs - No missing values, no holes, very good!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "618b738d",
      "metadata": {
        "id": "618b738d"
      },
      "source": [
        "### Feature distribution and boxplot by target\n",
        "\n",
        "To plot some of our features, we use a Python package called matplotlib, which we also already imported at the start (`import matplotlib.pyplot as plt`). To make figures with this package, the path is almost always the same:\n",
        "- plt.figure() -> This makes a new figure object, into which we can now put stuff.\n",
        "- \"The stuff\" -> This is where we actually draw the figure. We can change the labels on the axes (`plt.xlabel()` and `plt.ylabel()`), give the figure a title or subtitle (`plt.title()`), we could put dots into it (`plt.plot(x_coordinates, y_coordinates)`), bars, histograms, etc. etc. etc.\n",
        "- plt.show() -> This shows the figure and everything we put into it, sort of like print but for plots.\n",
        "\n",
        "There is a lot of \"stuff\", and we can't tell you about all of it here, so you will have to search for some of it on the internet yourself. However, everyone uses matplotlib, so you will definitely find functions you need just by googling or going here: https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n",
        "\n",
        "Now, let's look at some feature distributions and our targets a bit more closely. We start by looking at the feature _mean radius_. You can switch features by changing the feature name at the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12d4a60",
      "metadata": {
        "id": "b12d4a60"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature = \"mean radius\"\n",
        "\n",
        "plt.figure()\n",
        "X[feature].hist(bins=30)\n",
        "plt.title(f\"Histogram of {feature}\")\n",
        "plt.xlabel(feature)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "df.boxplot(column=feature, by=\"target\")\n",
        "plt.title(f\"Boxplot of {feature} by target\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"target (0 = malignant, 1 = benign)\")\n",
        "plt.ylabel(feature)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd1af32",
      "metadata": {
        "id": "0fd1af32"
      },
      "source": [
        "## Split train/test\n",
        "\n",
        "Now that we are a little more familiar with the data, let us get to modeling the target. As discussed in the lecture, we need to split the data into train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea4baab",
      "metadata": {
        "id": "6ea4baab"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=RNG\n",
        ")\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f8898d",
      "metadata": {
        "id": "f5f8898d"
      },
      "source": [
        "## Preprocessing pipelines\n",
        "\n",
        "The code below performs so-called *preprocessing*. For most datasets in the real world, preprocessing is one of the most important parts of machine learning. Depending on the dataset, task, and exact method of machine learning, preprocessing can include an almost infinite number of things you could do. Specifically for tabular data and older models like we will use today, the steps below are very common:\n",
        "- Make sure all data has the same data type (\"dtype\") - if one point of data had a radius of 12 and the other a radius of \"dog\", the machine learning model would immediately break\n",
        "- Make sure no data is missing - What is the average of the numbers \"1\", \"2\", \"3\" and \"_\"? We can't say, we don't know what the empty data point would have been. Our two options are to either replace the data point by inventing a good number - maybe the average, or median, both of which are \"2\" in this example. Or maybe we randomly pick one of the numbers. The technical term for this replacing/inventing is *Imputation*. Alternatively, we could also say \"Maybe I am making a mistake when creating data points like that\", and just throw out the missing data point.\n",
        "- Data normalization - A lot of the time, machine learners normalize their data, which means moving it to within a specific range and shape, typically a uniform distribution from 0 to 1 () or a normal distribution with mean 0 and variance 1. This is done because machine learning models, particularly modern ones, often work best when the input numbers are near zero. We will discuss why this is, when we start working with neural networks. Some examples:\n",
        "  - Normalization to [0, 1] looks like this:\n",
        "    \n",
        "    We start with the values [12, 2, 8, 4]\n",
        "  - We subtract the minimum (2) -> [10, 0, 6, 2]\n",
        "  - We divide by the maximum (10) -> [1, 0, 0.6, 0.2]\n",
        "  - Now all values are in the interval [0, 1]\n",
        "\n",
        "  - Normalization to a normal gaussian distribution $\\mathcal{N}(\\mu=0,\\sigma=1)$ looks like this:\n",
        "    \n",
        "    We start with the same values as before [12, 2, 8, 4]\n",
        "  - We subtract the mean (6.5) -> [5.5, -4.5, 1.5, -2.5]\n",
        "  - We divide by the standard deviation (4.435) -> [0.716039, -0.58585, 0.195283, -0.325472]\n",
        "  - Now the values have roughly 0 mean and variance\n",
        "\n",
        "All of the features we will use for machine learning today are numerical (numbers such as \"1\", \"2.5\", etc.). Categorical features would be something like a column called \"names\", with entries such as \"Alice\", \"Bob\", etc. - you can differentiate between them, but they don't have an obvious way of ordering them in a mathematical sense. The pipeline below contains preprocessing code for both numerical data, as well as categorical data, just in case.\n",
        "\n",
        "\n",
        "\n",
        "**Task 1 (1 point)**: Why is it potentially bad to impute data points? Do we need data imputation in our pipelines? Explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8537ae30",
      "metadata": {
        "id": "8537ae30",
        "outputId": "89c6d20d-e163-4380-ce5a-1b4b9b00423e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-919130419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumerical_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcategorical_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should be empty here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m numerical_pipe = Pipeline([\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"impute\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"median\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "numerical_columns = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_columns = X.select_dtypes(exclude=np.number).columns.tolist()  # should be empty here\n",
        "\n",
        "numerical_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", numerical_pipe, numerical_columns),\n",
        "    (\"cat\", categorical_pipe, categorical_columns)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d15f22",
      "metadata": {
        "id": "a8d15f22"
      },
      "source": [
        "## Baseline model: Majority Class\n",
        "\n",
        "We will now build our first machine learning models. For this week, we will still be using a lot of pre-built pieces that sklearn offers us. Later in the seminar, we will make some much stronger models ourselves, from scratch.\n",
        "\n",
        "As a simple baseline, we build a _model_ that always predicts the majority class. Any reasonable model should outperform this model by a wide margin, but its a good baseline to have - if your model *doesn't* beat this baseline later on, you know your model or training code is broken.\n",
        "\n",
        "We make a dummy model, and put it into a dummy pipeline. You can imagine a pipeline as a list of instructions - do this, then that and finally the other things. The fit function in a pipeline optimizes parameters of the model in the pipeline towards some goal or loss function - what it does exactly, we will look at a little later. The predict function takes the input for the model, computes the output and *doesn't* optimize anything.\n",
        "\n",
        "Our fit and predict functions right now don't do any clever machine learning yet; they only figure out what the most frequently correct answer is, and then \"predict\" that. In our case, we saw at the start that about 62.9% of our data were benign growth, so the model will predict ONLY benign. If we check the accuracy, it should be almost exactly that percentage (almost because we made random train and test sets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4367c7",
      "metadata": {
        "id": "1d4367c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "dummy_pipeline = Pipeline([(\"pre\", preprocess), (\"clf\", DummyClassifier(strategy=\"most_frequent\", random_state=RNG))])\n",
        "dummy_pipeline.fit(X_train, y_train)\n",
        "y_pred_dummy = dummy_pipeline.predict(X_test)\n",
        "print(\"y_pred_dummy:\", y_pred_dummy)\n",
        "\n",
        "# Accuracy baseline\n",
        "acc_dummy = accuracy_score(y_test, y_pred_dummy)\n",
        "\n",
        "print(f\"Baseline Accuracy: {acc_dummy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24a80ab",
      "metadata": {
        "id": "e24a80ab"
      },
      "source": [
        "## A Linear Model: Logistic Regression\n",
        "\n",
        "We now build our first *real* model. We use a linear model with the logistic loss function. This model is called **logistic regression**, because we try to model the *logits*, i.e., the log-odds of the probability that a given input belongs to the positive class.  \n",
        "\n",
        "Formally, we assume that the log-odds are a linear function of the input features:\n",
        "\n",
        "$$\n",
        "\\text{logit}(p) = \\log\\frac{p}{1 - p} = \\mathbf{w}^\\top \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "The logistic (sigmoid) function then maps these logits back to probabilities between 0 and 1:\n",
        "\n",
        "$$\n",
        "p = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n",
        "$$\n",
        "\n",
        "This allows us to perform binary classification by fitting the model parameters $\\mathbf{w}$ and $b$ to minimize the logistic loss over the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73df396d",
      "metadata": {
        "id": "73df396d"
      },
      "outputs": [],
      "source": [
        "# Like before, we make a pipeline, this time with a different model, the Logistic Regression\n",
        "logreg = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000, random_state=RNG))\n",
        "])\n",
        "\n",
        "# Like before, we fit and predict\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_lr = logreg.predict(X_test)\n",
        "y_proba_lr = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# And now we can check how well we did\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"LogReg Accuracy: {acc_lr:.3f}\")\n",
        "print(f\"LogReg F1:       {f1_lr:.3f}\")\n",
        "\n",
        "print(\"\\nClassification report (LogReg):\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=[\"malignant\", \"benign\"]))\n",
        "\n",
        "# We can also plot the Confusion Matrix and our Precision and Recall stats,\n",
        "# which sklearn can do for us very easily, with classes we have already imported.\n",
        "# (The plt.figure() part is already taken care of by these classes.)\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lr), display_labels=[\"malignant\",\"benign\"]).plot()\n",
        "plt.title(\"Confusion Matrix — Logistic Regression\")\n",
        "plt.show()\n",
        "\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_proba_lr)\n",
        "plt.title(\"Precision–Recall — Logistic Regression\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015e3fde",
      "metadata": {
        "id": "015e3fde"
      },
      "source": [
        "## A Non-Linear Model: Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb452478",
      "metadata": {
        "id": "eb452478"
      },
      "outputs": [],
      "source": [
        "\n",
        "tree = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=RNG))\n",
        "])\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = tree.predict(X_test)\n",
        "y_proba_dt = tree.predict_proba(X_test)[:, 1]\n",
        "\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "\n",
        "print(f\"Tree Accuracy: {acc_dt:.3f}\")\n",
        "print(f\"Tree F1:       {f1_dt:.3f}\")\n",
        "\n",
        "print(\"\\nClassification report (Decision Tree):\")\n",
        "print(classification_report(y_test, y_pred_dt, target_names=[\"malignant\", \"benign\"]))\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_dt), display_labels=[\"malignant\",\"benign\"]).plot()\n",
        "plt.title(\"Confusion Matrix — Decision Tree\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_proba_dt)\n",
        "plt.title(\"Precision–Recall — Decision Tree\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3945dddf",
      "metadata": {
        "id": "3945dddf"
      },
      "source": [
        "## A Tiny Bit of Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbac230",
      "metadata": {
        "id": "adbac230"
      },
      "outputs": [],
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
        "\n",
        "# Logistic Regression grid (accuracy-only)\n",
        "param_grid_lr = {\n",
        "    \"clf__C\": [0.1, 1.0, 10.0]\n",
        "}\n",
        "logreg_grid = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000, random_state=RNG))\n",
        "])\n",
        "gs_lr = GridSearchCV(logreg_grid, param_grid_lr, cv=cv, scoring=\"accuracy\", n_jobs=None)\n",
        "gs_lr.fit(X_train, y_train)\n",
        "print(\"Best LogReg params:\", gs_lr.best_params_, \"CV Accuracy:\", round(gs_lr.best_score_, 3))\n",
        "\n",
        "# Evaluate on test (accuracy)\n",
        "y_pred_lr = gs_lr.predict(X_test)\n",
        "print(\"Test Accuracy (best LogReg):\", round(accuracy_score(y_test, y_pred_lr), 3))\n",
        "\n",
        "# Decision Tree grid (accuracy-only)\n",
        "param_grid_dt = {\n",
        "    \"clf__max_depth\": [3, 5, 10],\n",
        "    \"clf__min_samples_leaf\": [1, 5, 20]\n",
        "}\n",
        "tree_grid = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=RNG))\n",
        "])\n",
        "gs_dt = GridSearchCV(tree_grid, param_grid_dt, cv=cv, scoring=\"accuracy\", n_jobs=None)\n",
        "gs_dt.fit(X_train, y_train)\n",
        "print(\"Best Tree params:\", gs_dt.best_params_, \"CV Accuracy:\", round(gs_dt.best_score_, 3))\n",
        "\n",
        "# Evaluate on test (accuracy)\n",
        "y_pred_dt = gs_dt.predict(X_test)\n",
        "print(\"Test Accuracy (best Tree):\", round(accuracy_score(y_test, y_pred_dt), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2ff217",
      "metadata": {
        "id": "2a2ff217"
      },
      "source": [
        "**Task 2 (1 point)**: Which model generalized better on this dataset? Elaborate why you think this is the case.\n",
        "\n",
        "**Task 3 (2 points)**: How do `max_depth` and `min_samples_leaf` change the decision tree's variance? Test this, and summarize your results.\n",
        "\n",
        "**Task 4 (1 point)**: Inspect the confusion matrices. If the cost of false negatives is high (i.e. we want to avoid them *really strongly*), how would you adjust the threshold? Is there a minimum/maximum to the threshold that you should not adjust it past? Test this, and explain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e32164-f564-447c-870d-258cd0610b93",
      "metadata": {
        "id": "a0e32164-f564-447c-870d-258cd0610b93"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}